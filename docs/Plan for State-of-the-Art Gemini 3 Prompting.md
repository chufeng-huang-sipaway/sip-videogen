# Plan for State-of-the-Art Gemini 3 Prompting

## 1. Current Agent Prompting Setup (Gemini 3 Image Generation)

Our app’s agent already integrates with Google’s **Gemini 3.0 Pro** image model via the `generate_image` tool[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/prompts/advisor.md#L20-L28)[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/tools.py#L2557-L2565). The agent is a conversational LLM (GPT-5.1) that uses a **skills** framework to produce high-quality prompts for image generation:

- **Image Prompt Engineering Skill:** A dedicated skill (`image-prompt-engineering`) provides guidelines on crafting prompts. It emphasizes **natural, narrative descriptions** over keyword lists[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/skills/image_prompt_engineering/SKILL.md#L37-L44). Every prompt should cover five key elements – **Subject, Setting, Style, Lighting/Mood,** and **Composition**[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/skills/image_prompt_engineering/SKILL.md#L48-L56)[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/skills/image_prompt_engineering/SKILL.md#L68-L76) – in full sentences. This ensures specificity and context (e.g. *“A frosted glass bottle with a copper cap… on a marble countertop in a bright kitchen… shot as an editorial photo with soft morning light, 45° overhead angle”*). The agent is instructed to always apply this formula *before* calling `generate_image`[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/prompts/advisor.md#L28-L36).
- **Immediate Action:** The agent’s system prompt makes it clear that when a user requests an image, the agent **should not** respond with long explanations or suggestions – it should **immediately call** the `generate_image` tool with a well-crafted prompt[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/prompts/advisor.md#L5-L13)[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/prompts/advisor.md#L46-L55). For example, if the user says *“Create a lifestyle image showing someone using this product,”* the agent will internally expand that into a detailed scene description (using the skill guidelines) and then call `generate_image`[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/prompts/advisor.md#L52-L59).
- **Reference Image & Identity Preservation:** If the user provides a reference image (e.g. the actual product photo), the agent passes it to `generate_image(reference_image=…, validate_identity=True)` to ensure the generated image preserves the exact appearance of the product[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/skills/image_prompt_engineering/SKILL.md#L125-L134)[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/prompts/advisor.md#L100-L108). The skill explicitly says to *“describe what to KEEP (the product) and what to CHANGE (the setting)”* when using a reference[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/skills/image_prompt_engineering/SKILL.md#L134-L141). Our agent already follows up on this: **any follow-up requests** (“more like this,” “try a different angle”) will reuse the **same reference image** with `validate_identity=True` so the product stays identical[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/prompts/advisor.md#L120-L128)[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/prompts/advisor.md#L140-L148). This addresses the critical requirement that *“every pixel matters”* for product accuracy[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/prompts/advisor.md#L184-L193)[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/prompts/advisor.md#L196-L199) – if even subtle material or color details change, it’s considered a failure.
- **Multi-Product Scenes:** The agent can feature 2–3 distinct products in one image. In these cases it uses `product_slugs=[…]` with Gemini’s multi-reference capability (our tool auto-loads each product’s images and enforces multi-object identity validation)[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/tools.py#L2592-L2600)[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/tools.py#L2604-L2612). The agent’s prompt guidelines are extremely strict here: it must list each product with **exact materials, colors, and distinctive features**, differentiating them clearly (e.g. “Product A: **frosted glass** bottle with **copper cap**…; Product B: **matte black** jar with **textured surface**…”)[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/prompts/advisor.md#L170-L178). The prompt explicitly warns that each product **must appear exactly as its reference** – *“Preserve all materials, colors, textures, and proportions exactly. If you fail even by one pixel… the generation fails.”*[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/prompts/advisor.md#L182-L190). This ensures Gemini knows accuracy is paramount. Our agent already implements these best practices for multi-product images.
- **Brand & Template Context:** Before generation, the agent can inject brand identity (color palettes, style guidelines) and template layout constraints. It uses tools like `load_brand()` to retrieve brand styling info[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/prompts/advisor.md#L202-L210) and supports `template_slug` parameters to impose a predefined layout (with **strict** or **loose** adherence)[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/prompts/advisor.md#L222-L231)[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/prompts/advisor.md#L238-L247). In strict mode, the agent knows to *only replace the product in the exact locked layout*[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/prompts/advisor.md#L230-L239)[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/prompts/advisor.md#L232-L240), whereas in loose mode it can allow minor composition shifts. This templating system is our current approach to **structural control** over layouts. (Notably, the template’s JSON constraints are applied under the hood, rather than sending a sketch image to Gemini[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/prompts/advisor.md#L266-L274).)

Overall, the current architecture gives the agent the *capability* to prompt Gemini 3 effectively – it has the tools and a strong baseline skill for prompt quality. Next, we’ll enhance this by integrating **Nano-Banana Pro** prompt techniques to make our agent truly state-of-the-art.

## 2. Nano-Banana Pro Prompting Techniques – Key Insights

Google’s *Nano-Banana Pro* (Gemini 3 Pro image model) introduces advanced prompting strategies beyond basic description. The official prompting guide emphasizes that Nano-Banana Pro is a **“thinking” model** – it understands intent, physics, and context at a high level[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=Section 0%3A The Golden Rules,of Prompting). Simply put, we should *“stop using tag soups… and start acting like a Creative Director”*[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=Nano,acting like a Creative Director). Key techniques from the guide include:

- **Golden Rules of Prompting:**
  1. **Edit, Don’t Re-roll:** Treat prompt refinement like a conversation. If an initial image is 80% correct, **don’t start from scratch** – instead, instruct the model on what to change[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=1. Edit%2C Don't Re,the specific change you need). Nano-Banana Pro excels at iterative edits based on feedback (thanks to its “thinking” ability).
  2. **Use Natural Language & Full Sentences:** Prompts should read like you’re briefing a human artist[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=2,proper grammar and descriptive adjectives). Grammar and complete sentences help the model parse complex intent. (For example, *“A cinematic wide shot of a futuristic sports car speeding through a rainy Tokyo street at night…”* yields far better results than *“cool car, neon, city, night 8k”*[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=2,proper grammar and descriptive adjectives).) This aligns with what our skill already does – narrative description instead of comma-separated keywords[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/skills/image_prompt_engineering/SKILL.md#L37-L44).
  3. **Be Specific and Descriptive:** Vague prompts give generic images. The guide stresses precise details: define the **subject** (“not just *a woman*, but *a sophisticated elderly woman in a vintage Chanel suit*”), **textures** and materials (“matte finish, brushed steel”), etc[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=3,the lighting%2C and the mood). Again, our existing 5-point prompt formula covers this well[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/skills/image_prompt_engineering/SKILL.md#L48-L56)[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/skills/image_prompt_engineering/SKILL.md#L60-L68), though we can reinforce material and texture cues even more.
  4. **Provide Context (the “Why/For Whom”):** Because the model “thinks,” adding context makes prompts more effective[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=4. Provide Context (The ,it make logical artistic decisions). E.g. saying *“… for a Brazilian high-end gourmet cookbook”* cues the model to make stylistic choices appropriate for that audience[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=4. Provide Context (The ,it make logical artistic decisions). Our skill already advises stating the image’s purpose or usage[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/skills/image_prompt_engineering/SKILL.md#L86-L94)[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/skills/image_prompt_engineering/SKILL.md#L107-L114), which is great for context.
- **Text Rendering & Infographics:** Nano-Banana Pro can render legible, stylized text within images – a traditional weak spot for generative models. Best practice is to **clearly specify any text in quotes** and describe its style[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=Nano,complex information into visual formats). For infographics or visual explanations, you can even ask the model to **synthesize information visually** (e.g. “compress this data into a diagram”). In short, treat the model like it can layout text elements logically.
- **Character Consistency:** The model is strong at maintaining character appearance across images in a sequence or storyboard. The guide demonstrates that if you give a character a name or detailed description and stick to it, the model will keep them consistent through multiple frames[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=You can generate sequential art,fake leaks of upcoming films)[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=Example Prompt%3A). This is useful for brand mascots or recurring actors in a campaign. It implies our agent should reuse the same descriptors (and reference images if available) whenever continuity is needed.
- **Grounding with Knowledge:** Nano-Banana Pro has world knowledge and can even integrate Google Search results internally. While our agent doesn’t directly perform web searches, we can trust the model’s knowledge for common objects or locales. For example, if the prompt mentions a famous landmark or a specific animal breed, the model likely knows how it looks. We should still describe such elements fully, but we need less worry about factual visuals (e.g. it will correctly render Eiffel Tower by name). If extremely precise reference is needed, the user or agent could still provide an image, but often the model’s training suffices.
- **Advanced Edits via Prompt:** Perhaps the biggest leap is **in-painting and image editing through pure language instructions**. The guide shows we can **tell the model what to change** in an uploaded image without manual masking[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=The model excels at complex,Style Swapping). For instance: *“Remove the tourists from the background and replace them with cobblestones and storefronts that match the scene.”* Nano-Banana will intelligently fill in the background accordingly[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=> Object Removal %26 In,that match the surrounding environment). Similarly, *“Colorize this black-and-white manga panel with vibrant anime colors…”* will yield a colorized image[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=,consistent with their official colors). The agent should leverage this by **using the `reference_image` parameter for the input image and describing the desired changes in natural language**. We don’t need a new tool – just a new prompting pattern: e.g. `generate_image(prompt="Remove the blemishes and restore this photo to HD quality", reference_image="uploads/old_photo.jpg")`. No pixel-level instructions needed; the model infers the rest[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=Best Practices%3A).
- **Dimensional Translation (2D ↔ 3D):** Nano-Banana Pro can convert flat designs to 3D visuals and vice versa[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=5,3D)[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=Example Prompts%3A). This is useful if, say, a brand has a 2D schematic of packaging and wants a 3D render, or if they want a stylized sketch from a 3D scene. The prompting approach is to explicitly request the conversion and provide the reference image. For example: *“Based on the uploaded floor plan (reference), generate a photorealistic interior design board…”*[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=Example Prompts%3A). Our agent could detect such requests and include the source diagram as a `reference_image` with a detailed prompt for the new format (ensuring to mention layout details that should carry over). The model will handle mapping 2D to 3D (or vice versa) internally.
- **High-Resolution & Texture Emphasis:** The model natively supports 4K output[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=Nano,format prints), which we already exploit (our `generate_image` defaults to `"4K"` size[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/tools.py#L110-L118)[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/tools.py#L188-L196)). The guide suggests explicitly mentioning when high fidelity is needed: e.g. *“every strand of moss and beam of light rendered in pixel-perfect detail”* for a 4K scene[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=,fidelity details (imperfections%2C surface textures). In practice, our agent can continue to default to 4K and should include texture details in prompts (e.g. *“intricate surface detail, visible grain”*) when quality is critical. We should also keep an eye on performance (4K generation is heavier); but since accuracy is our priority, this is acceptable.
- **“Thinking” Mode and Complex Reasoning:** The model can perform intermediate reasoning – e.g. solving a math equation on a whiteboard step-by-step in an image[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=7). This might be outside typical product shots, but it tells us the model can follow multi-step logical instructions in a single prompt. For us, this reinforces that we can ask for **complex compositions** (e.g. *“an infographic breaking down the product’s ingredients with each component labeled and illustrated”*) and trust the model to logically arrange those elements. The agent should not shy away from detailed multi-part prompts; Nano-Banana Pro can handle them.
- **One-Shot Storyboards & Multi-Image Sequences:** The guide even shows you can prompt the model to generate a **series of images** with consistent narrative and characters[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=You can generate sequential art,fake leaks of upcoming films). In our app, multi-image output would usually be handled by successive `generate_image` calls (and possibly our `propose_images` tool for variations). We likely won’t ask the model to emit a 9-frame storyboard in one go (since our UI and tooling expect one image per call), but this capability suggests our agent can coordinate multi-step storytelling: e.g. by generating one image at a time and ensuring consistency. The agent should maintain the **same prompt style and references** across the sequence (or even instruct the model, “continue the story…”). Essentially, treat sequential generations as a continuous creative session.
- **Strict Layout Control via Image Inputs:** Nano-Banana Pro allows using an input image (sketch, wireframe, grid) purely to **dictate layout/composition**[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=Input images aren't limited to,layout into a polished asset). For designers, this is *huge*: we can sketch a layout and have the model fill it in with polish. Our system already supports layout via templates (JSON constraints), but we can also expose this more directly. For instance, if a user sketches an ad layout and uploads it, the agent can call: `generate_image(prompt="Create a final ad for [product] following this sketch exactly.", reference_image="uploads/sketch.png")`. The guide’s best practices: provide the sketch and say “following this sketch”[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=Input images aren't limited to,layout into a polished asset)[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=,product] following this sketch). The model will place elements as drawn. This could simplify workflows for custom layouts that aren’t in our template library. Similarly, a wireframe screenshot could be given to generate a high-fidelity UI mockup[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=,based games or LED displays)[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=,product] following these guidelines). We should incorporate support for these scenarios.

With these insights in mind, we will update our agent to fully capitalize on Nano-Banana Pro’s strengths, while leveraging our existing robust framework.

## 3. Upgrading the Agent with Nano-Banana Pro Techniques

**a. Enhance the Prompt Engineering Skill with New Guidelines:** We will update the **`image-prompt-engineering` SKILL.md** to incorporate Nano-Banana Pro’s prompting strategies on top of our current 5-point formula. Key additions:

- **Reiterate Natural Language & Context:** Emphasize (again) *full-sentence prompting* and including the “why/for whom.” Our skill already covers this, but we’ll add examples from the Nano-Banana guide to reinforce. For instance, show a before/after prompt (like the “cool car” vs narrative car example)[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=2,proper grammar and descriptive adjectives) to illustrate how much better a descriptive sentence is. This will solidify the agent’s avoidance of any terse “tag soup” style inputs.
- **More Texture and Material Details:** Augment the **Subject** and **Style** sections of the skill with guidance to include **material textures and imperfections** when relevant (from Golden Rule #3). E.g. advise phrases like “soft velvet upholstery,” “fingerprint-smudged glass” if appropriate. Nano-Banana’s ability to render fine details means the agent can push for realistic texture cues[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=3,the lighting%2C and the mood). We’ll incorporate these examples so the agent consistently thinks in terms of surface detail.
- **Quoting Text for In-Image Labels:** Add a note under “Critical Do’s” about text: whenever the user needs specific text (e.g. a slogan or UI label) in the image, the agent should put that text in **double quotes** in the prompt[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=,layer with its flavor profile). We already had “Describe text precisely”[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/skills/image_prompt_engineering/SKILL.md#L109-L116); now we’ll explicitly mention quoting and perhaps all-caps for exact words (as our skill’s example does with **'BLOOM'**[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/skills/image_prompt_engineering/SKILL.md#L109-L114)). This will improve the model’s success at rendering readable text in things like product labels, infographics, or ads.
- **Prompting Edits to Images:** Introduce a section on **“Image Editing Prompts.”** Explain that if the task is to modify an existing image (remove background, change color, restore damage, etc.), the agent should: 1) use the input image as `reference_image`, 2) write the prompt as a **natural instruction** of the desired edit[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=Best Practices%3A). For example: *“Remove the background and place the product on a pure white backdrop,”* or *“Colorize this black-and-white image in realistic colors.”* The skill will caution **not** to list tedious pixel-level operations – just describe the outcome, since the model will handle the low-level details. We can cite the guide’s object removal example to illustrate this style[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=> Object Removal %26 In,that match the surrounding environment).
- **Dimensional Conversion & Style Transfer:** Add guidance for when a user provides one form of imagery and wants another (2D ↔ 3D, or style change). The agent should explicitly mention the transformation. E.g. *“Using the uploaded line drawing as a blueprint, create a full-color 3D render of the product,”* or *“Turn this 3D scene into a flat 2D illustration with the same composition.”* By including phrases like “turn this into…” or “based on the uploaded X, generate Y,” we tap into the model’s strength in translation tasks[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=Example Prompts%3A). We’ll include an example prompt similar to the floor-plan conversion in the skill doc.
- **Structural/Layout Control:** Even though we have templates, we’ll prepare the agent for cases where a user *sketches a layout*. The skill will note: *If an arbitrary image is attached as a “layout” or “sketch,” use it as `reference_image` and say “follow this layout/sketch exactly.”*[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=Input images aren't limited to,layout into a polished asset)[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=,product] following this sketch) This way, the agent knows it can directly leverage Nano-Banana’s layout-following ability when a template JSON isn’t available. (This acts as a fallback or user-driven template approach.) We will make sure the skill’s trigger words include “sketch”, “wireframe”, “layout” so that these instructions load at the right time.
- **Iterative Refinement Workflow:** While our advisor instructions already discourage the agent from over-explaining and encourage quick iterations[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/prompts/advisor.md#L214-L222)[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/prompts/advisor.md#L74-L82), we’ll explicitly incorporate *“Edit, don’t re-roll”* into the skill or system prompt. This means reminding the agent that **on user feedback**, it should modify the last prompt rather than starting a brand new concept. For instance, if the user says “Can you make the lighting warmer?”, the agent should **reuse the previous prompt and just adjust the lighting part**, keeping the rest (and keeping `reference_image` if one was used). We can build a small internal routine for the agent: fetch the last generation’s prompt from metadata (we store `original_prompt` in our image metadata) and tweak it. However, even without new code, the LLM can do this if instructed: we’ll train it via the skill to handle such follow-ups by literally following the user’s instruction in prompt changes instead of going wild in a new direction[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=1. Edit%2C Don't Re,the specific change you need). This ensures consistency and faster convergence to what the user wants.

**b. Broaden Trigger Detection for Image Tasks:** We will refine how the agent detects when to load the image-prompting skill. Currently it triggers on obvious words (“image, photo, render, picture, etc.”)[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/skills/image_prompt_engineering/SKILL.md#L4-L13). We should **add keywords like “infographic”, “diagram”, “sketch”, “layout”, “mockup”** to that trigger list. This way, if the user says “Create an infographic” or “Here’s a sketch of an ad,” the agent will still load the prompt-engineering skill and apply all these guidelines. Expanding the trigger list is a simple YAML edit in SKILL.md frontmatter, but crucial for full coverage.

Additionally, in our agent code, we should ensure that if a user attaches an image file without explicitly saying “image” (for example, user just says “like this” with an attachment), the agent still treats it as an image generation context. One approach: our conversation UI already shows “Current Context” with attachments; we can have the agent consider any image attachment as an implicit trigger for image generation skill. This might involve updating `find_relevant_skills` logic to check for attached image types, or simply rely on the LLM’s reasoning (our system prompt lists skills and says *“when relevant, load and follow its instructions”*[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/skills/registry.py#L158-L167) – the model can likely infer an attached `.png` means image task). But to be safe, we can programmatically flag the skill as relevant when an image file is present in user input.

**c. Leverage Multi-Modal Inputs More Aggressively:** Nano-Banana Pro thrives with reference images. Our agent already uses references for products and style; we can extend this:

- If the user provides **multiple reference images** (e.g. a mood board of 2-3 inspiration images), we can pass all of them. Our `generate_image` supports a list of `reference_images` (and groups them per product for multi-product). For style references that aren’t products, we could call `generate_image` once per reference to get bytes, but the GenAI API can also take multiple images as context. We’ll need to confirm if `google.genai` supports multiple generic references in one call. The blog implies yes – e.g. you could provide a style image and a layout image together (one as a sketch layout, one as a color/style inspiration). To implement: we might update `_impl_generate_image` to accept a list of arbitrary reference images (currently it mainly handles multi-product grouping). Since the API’s `contents` structure can include multiple images with labels[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/tools.py#L156-L165)[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/tools.py#L188-L196), we could allow the agent to attach images with roles via prompt text (e.g. “Reference style image:” label). This is a more technical change, so as an interim, we’ll instruct the agent to prioritize the **most relevant reference** if only one can be used at a time. But exploring multi-ref prompts is on the roadmap (perhaps using the grouped reference mechanism creatively).
- Encourage the agent to use **`product_slug` shortcuts** when applicable. Our skill already says if a product is attached in context, use `product_slug` rather than manually inserting the reference path[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/prompts/advisor.md#L156-L164). This auto-activates identity validation and loads all product images for the model. We should ensure this guidance remains highlighted. The agent should also combine `product_slug` and `template_slug` for attached templates[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/prompts/advisor.md#L274-L282) so that both product identity and layout are enforced together. This is already in our instructions[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/prompts/advisor.md#L276-L285); we’ll double-check the agent follows it. Essentially, by using these parameters, the agent offloads a lot of heavy lifting (the code will append the template constraints, etc.).

**d. Refine the Validation & Iteration Loop:** The “one pixel off” mandate means our current validation logic must be rock-solid. We have a `generate_with_validation` loop that re-generates until the product in the output matches the reference (or max retries)[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/tools.py#L38-L46)[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/tools.py#L68-L77). We should continue using this. To further improve: if after a couple attempts the model is failing a particular detail (say the color tone is slightly off), we could have the agent **augment the prompt** for the next attempt by explicitly stating that detail. For example, if the bottle’s cap came out too dark, the agent could insert “cap must be bright copper-orange, exactly as reference” and try again. This kind of dynamic prompt tweaking could be done by analyzing the validation feedback. However, this might be over-engineering for now – our existing approach simply picks the best attempt and warns if not perfect[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/tools.py#L44-L52). For the scope of this plan, we’ll rely on the current validation and the stronger upfront prompting to reduce mismatches.

That said, one concrete addition: after generating, the agent can use the **model’s conversational nature to do a quick self-check**. Nano-Banana Pro is said to have “thinking” interim steps[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=7); while we can’t see those, we might exploit the model’s understanding via a follow-up question in system or an automated tool. For instance, after generation, our code could use an image-analysis function (perhaps powered by Gemini’s image-to-text capabilities) to describe the output and verify key attributes. We do have `advisor.image_analyzer` – possibly used for template analysis or validation. If such exists (maybe `sip_videogen/advisor/validation.py` or `image_analyzer.py`), we can integrate it to confirm the output’s colors/materials match the product spec. This would add a safety net beyond pixel comparison. However, if this is too much, we ensure at least the multi-attempt validation is logged and any warning is clearly communicated to the user (“The image was generated but some details might not match exactly.”).

**e. Skill Architecture vs Over-Engineering:** We will *not* explode the number of skills for every sub-topic (e.g. a separate “infographic skill” or “editing skill”), as that could complicate maintenance and confuse the agent. Instead, we’ll keep **one unified image prompt engineering skill**, enriched with sections for these advanced scenarios. The agent can handle nuanced instructions within one skill document – it’s actually easier for the LLM to have a comprehensive guide in one place. Our system is designed to load at most a couple of skills per request (to conserve context)[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/agent.py#L698-L707); keeping everything under the image skill ensures the agent always has the full picture when working on an image task. This is sufficient and avoids over-engineering new skill selection logic.

One exception: if we find the skill file growing too large, we might consider splitting out a very distinct domain (for example, “logo design” might be separate as it involves vector style, etc., and indeed we have a `logo_design` SKILL.md in the repo). But for Nano-Banana Pro prompting, it’s all closely related, so a single skill update suffices.

**f. Testing and Iteration:** Once we implement these changes, we’ll thoroughly test with a variety of requests to ensure the agent’s behavior is optimal:

- **Basic product shot** (“Generate a hero image of our shampoo bottle for a web banner”) – check that the agent’s prompt follows the full formula and uses brand context and `product_slug` correctly. The output should match the product exactly (we’ll verify no validation warnings). We expect a narrative prompt with all elements (subject detail, environment, lighting, etc.) – essentially making sure our original capabilities remain solid.
- **Infographic with text** (“Make an infographic for this product’s features”) – watch that the agent puts feature text in quotes and describes a clear layout (e.g. “a labeled diagram with icons”). Ensure it calls `generate_image` and not something else. We’ll look at the image to see if text is legible and properly placed, adjusting prompt phrasing if needed (like maybe specifying font styles as we already advise[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/skills/image_prompt_engineering/SKILL.md#L109-L116)).
- **Image editing scenario** (“Here is an image of our product on a cluttered shelf, remove the background”) – after attaching an image, see that the agent uses it as reference and says “remove the background” in prompt. The result should be the product on a plain background. If it fails, we might need to explicitly add “white background” to prompt, etc. We’ll refine the skill instructions accordingly.
- **Multi-product group shot** (attach 2–3 products, ask for a group photo) – verify the agent lists each product with required specificity and uses `product_slugs` behind the scenes. The prompt should include the “Feature EXACTLY these products: 1… 2… etc” format from our guidelines[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/prompts/advisor.md#L180-L188). The outputs should show all products accurately. We’ll confirm that our updated instructions about differentiation and emphasis on *exact* appear in the agent’s prompt (they should, since the advisor.md already had those and the agent presumably considers them when products are attached).
- **Layout sketch** (“Convert this sketch to a final ad”) – attach a rough layout drawing and see if the agent uses it as reference_image with a prompt like “follow this sketch layout exactly.” Because this overlaps with our template system, we’ll test both approaches: with a known template vs with an arbitrary sketch. If the agent hesitates or ignores the sketch, we may adjust triggers or add logic to treat any unknown image as a layout reference by default. The goal is that the agent trusts the model to interpret the sketch.
- **Dimensional translation** (“Here’s our bottle design drawing, generate a 3D mockup”) – attach a 2D design. Expect agent to use it as reference and prompt the conversion. The output should be a realistic 3D-rendered bottle. If results aren’t satisfactory (maybe the model misses some label detail), we might need the agent to explicitly describe those details in prompt (since a line drawing might not convey color – the agent should read any text in the drawing or get specs from product data). We’ll refine either by instructing the agent to merge product attributes with the reference (e.g. *“following the sketch (shape and layout) but apply the product’s actual colors and label text”*).

Each test will inform if further tuning is needed. The beauty is that *most changes are in prompt content*, which we can tweak quickly without major code refactoring. Our skill updates and a few trigger adjustments should cover it. The underlying tool (`generate_image` with Gemini 3 API) remains the same reliable workhorse.

**g. Continuous Learning and Updates:** Nano-Banana Pro is cutting-edge, so best practices might evolve (Google could update the model or release new guidance). We’ll keep an eye on official Google AI communications. Our agent is structured to make updates easy: the system prompt and skill files can be edited without retraining any model. We should plan periodic reviews of prompt effectiveness. For example, if we notice the agent’s prompts are still not yielding perfect text in images, we might incorporate new tips (maybe the community finds that certain keywords like “typography” help, etc.). We’ll also gather user feedback from our app – if users frequently request edits (like “the color is slightly off”), that signals we should preemptively include those details in initial prompts.

Finally, as a “sanity check” against over-engineering: each addition should be justified by a clear need. Our plan focuses on **practical enhancements** that directly improve image fidelity and variety. We avoided drastic architectural changes (like building a separate image-editing pipeline or training custom models) since Gemini 3 + prompt engineering is powerful enough. By concentrating on skill instructions and correct tool usage, we keep the solution maintainable.

In summary, this plan bolsters our agent’s prompting capabilities with the **Nano-Banana Pro techniques** – ensuring it writes the most effective prompts possible and fully leverages the model’s advanced features. With these updates, our agent will behave like a top-notch creative director: producing on-target images of products with uncompromising accuracy (no off-brand pixels!), using iterative refinement, and exploiting Gemini’s new strengths like text rendering, layout control, and intelligent editing. This will position our app’s image generation as truly **state-of-the-art** – minimizing errors that could harm the brand, and maximizing the visual quality and relevance of every generated image.

**Sources:** The plan above is informed by our codebase’s current image prompting guidelines[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/skills/image_prompt_engineering/SKILL.md#L37-L44)[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/prompts/advisor.md#L182-L190) and Google’s official Nano-Banana Pro prompting guide[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=2,proper grammar and descriptive adjectives)[dev.to](https://dev.to/googleai/nano-banana-pro-prompting-guide-strategies-1h9n#:~:text=4,Colorization), as well as integration details from our `generate_image` implementation[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/tools.py#L2570-L2578)[GitHub](https://github.com/chufeng-huang-sipaway/sip-videogen/blob/53a3c641f9b2aea16ee529ef78bd281aea84ce2b/src/sip_videogen/advisor/tools.py#L533-L541). All cited references were used to ensure accuracy and to ground the recommended improvements in proven practices.